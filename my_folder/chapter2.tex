\chapter{Разработка метода, алгоритма, модели исследования} \label{ch2}

\section{Постановка задачи} \label{ch2:method_selection}

Для построения системы автоматического анализа коммитов GitHub каждое изменение представлено в виде вектора числовых признаков. Формально определим вектор признаков коммита как:
\begin{equation}
	c_i = \{a_i, d_i, t_i, f_i, \tau_i\, \gamma_i\},
\end{equation}

где:
\begin{itemize}
	\item \( a_i \) --- количество добавленных строк кода,
	\item \( d_i \) --- количество удаленных строк кода,
	\item \( t_i \) --- общее число изменений (сумма добавленных и удаленных строк),
	\item \( f_i \) --- количество измененных файлов,
	\item \( \tau_i \) --- время с момента последнего коммита.
	\item \( \gamma_i \) --- оценочная сложность изменений (суммарная цикломатическая сложность изменённых файлов).
\end{itemize}

Выбранное представление компактно описывает основные характеристики коммита и пригодно для обработки методами машинного обучения. Такие метрики широко используются в исследованиях по предсказанию дефектов коммитов: например, установлено, что с увеличением числа добавленных строк (\( a_i \)) растёт вероятность дефектного изменения.

Признак \( \gamma_i \) дополнительно учитывает сложность правок; например, при прочих равных более «тяжёлые» изменения могут требовать повышенного внимания. Таким образом, вектор \( c_i \) обеспечивает информативное и сжатое описание каждого коммита, пригодное для последующей автоматической обработки.

\section{Кластерный анализ коммитов} \label{ch2:problem_formulation}

Представленные вектора \( c_i \) рассматриваются как точки в \( R^5 \). Для первичной разметки коммитов на «нормальные» и «аномальные» применён метод кластеризации k-средних. Алгоритм k-means разбивает множество наблюдений на k кластеров, приписывая каждую точку кластеру с ближайшим центройдом. Выбирая небольшое число кластеров (например, k = 2), мы автоматически выявляем естественные группы коммитов по схожести признаков. Коммит считается потенциально аномальным, если он попадает в кластер с необычным центроидом или малой плотностью: например, кластер, содержащий коммиты с экстремально большим числом добавленных/удалённых строк или затронутыми файлами, трактуется как «аномальный».

Использование кластеризации вместо ручной установки порогов обладает рядом преимуществ:
\begin{itemize}
	\item Адаптивность. Границы между нормальными и аномальными коммитами определяются по распределению реальных данных, а не зависят от заранее заданных эвристик.
	\item Объективность. Модель сама выявляет естественные группы, устраняя субъективность эксперта при выборе порогов.
	\item Масштабируемость. Алгоритм эффективно обрабатывает большие массивы данных и может быть переиспользован без ручной перенастройки порогов.
\end{itemize}

Таким образом, кластеризация позволяет автоматически настроить «пороги аномальности» на конкретный набор коммитов, что делает обнаружение аномалий более адаптивным и надежным по сравнению с простыми эвристическими критериями.

\section{Классификация коммитов методом глубокого леса} \label{ch2:algorithm_selection}

После разметки коммитов на нормальные и аномальные на соответствующих метках обучается классификатор. В качестве модели используется глубокий лес (CascadeForestClassifier), основанный на идеях алгоритма gcForest. Это ансамблевый алгоритм, реализующий каскадную структуру из случайных лесов. На каждом уровне каскада обучаются несколько лесов решений; выходы лесов предыдущего уровня дополняют признаки для следующего уровня. Такая многоуровневая архитектура позволяет глубоко изучать представления данных без рекуррентный нейронный сетей.

Классификатор CascadeForestClassifier обучается на размеченных данных {\( c_i \), \( y_i \)} - где \( y_i \) - метка класса «нормальный/аномальный». После обучения модель выдаёт для нового коммита оценку вероятности его принадлежности к аномальному классу, то есть риска изменения. Численность уровней каскада может определяться автоматически (по критерию прироста качества), что позволяет глубокой модели хорошо работать при небольшом объёме обучающих данных, кроме того, глубокий лес имеет сравнительно мало гиперпараметров и демонстрирует высокую стабильность результатов.

Использование глубокого леса на размеченных данных даёт возможность уточнить исходную разметку кластеризацией: модель учитывает все признаки в совокупности и вырабатывает более точные границы между нормальными и аномальными коммитами. В результате этой стадии получается классификатор, который для каждого нового коммита вычисляет риск его аномальности.


\section{Заключение} \label{ch2:conclusion}

В этой главе сформулирована задача представления коммита в виде числового вектора признаков и описана двухэтапная схема автоматической классификации. Сначала при помощи алгоритма k-means выполняется первичное разбиение коммитов на «нормальные» и «аномальные» на основе их статистических характеристик. Затем на полученных метках обучается модель глубокого леса, уточняющая классификацию и прогнозирующая риск новых коммитов. Полученные оценки аномальности коммитов образуют основу для последующей генерации корректирующих действий (CAPA).
